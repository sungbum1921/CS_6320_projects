# Assignment 1 â€” N-gram Language Models

## ğŸ“˜ Overview
This assignment implements **Uni-gram** and **Bi-gram** language models to analyze text corpora and estimate sequence probabilities.  
You will also experiment with **add-k smoothing** and evaluate model quality using **Perplexity (PP)** on held-out data.

---

## ğŸ§  Objectives
1. Implement probabilistic language models based on **n-gram frequency counts**.  
2. Apply **smoothing techniques** to handle unseen word combinations.  
3. Compute **sentence probabilities** and **perplexity scores** on validation data.  
4. Compare model performance across different smoothing constants and n-gram orders.

---

## ğŸ“‚ Files Included
| File | Description |
|------|--------------|
| `train.txt` | Training corpus used to build n-gram counts. |
| `val.txt` | Validation corpus used to compute perplexity. |
| `assignment1_new.ipynb` | Jupyter notebook implementing the full pipeline (training, smoothing, evaluation). |

---

## âš™ï¸ Implementation Summary

### 1ï¸âƒ£ Uni-gram Model
Estimates word probabilities as:  
> **P(wáµ¢) = count(wáµ¢) / N**  
> where **N** is the total number of tokens in the training corpus.

---

### 2ï¸âƒ£ Bi-gram Model
Estimates conditional probabilities as:  
> **P(wáµ¢ | wáµ¢â‚‹â‚) = count(wáµ¢â‚‹â‚, wáµ¢) / count(wáµ¢â‚‹â‚)**  

This measures how likely word *wáµ¢* follows *wáµ¢â‚‹â‚* based on observed pairs.

---

### 3ï¸âƒ£ Add-k (Laplace) Smoothing
Handles zero probabilities using a constant *k > 0*:  
> **P(wáµ¢ | wáµ¢â‚‹â‚) = (count(wáµ¢â‚‹â‚, wáµ¢) + k) / (count(wáµ¢â‚‹â‚) + kV)**  
> where **V** is the vocabulary size.

Smoothing ensures that unseen word pairs receive small non-zero probabilities,  
preventing total probability collapse for unseen events.

---

## ğŸ§© Experiment Plan
| Model | Smoothing k | Evaluation Metric | Expected Trend |
|--------|--------------|------------------|----------------|
| Uni-gram | 0 | High PP | Baseline |
| Bi-gram | 0 | Moderate PP | Better than Uni-gram |
| Bi-gram | 0.1, 0.5, 1.0 | PP decreases | Optimal k near small positive value |

---

## ğŸš€ How to Run
1. Open `assignment1_new.ipynb` in Jupyter Notebook or VSCode.  
2. Run all cells in order.  
3. The notebook will:
   - Build n-gram counts from `train.txt`
   - Apply smoothing
   - Compute perplexity on `val.txt`
4. The output will show tables comparing PP across settings.

---

## ğŸ§¾ Example Output
| Model | k | Perplexity |
|--------|---|-------------|
| Uni-gram | â€“ | 823.7 |
| Bi-gram | 0 | 214.5 |
| Bi-gram | 0.1 | 199.2 |
| Bi-gram | 1.0 | 205.4 |

---

## âœ¨ Discussion Points
- Analyze how **smoothing** improves generalization on unseen words.  
- Compare **Uni-gram vs Bi-gram** behavior on rare tokens.  
- Comment on trade-off between model complexity and data sparsity.

---

## ğŸ“š References
- Jurafsky & Martin, *Speech and Language Processing*, 3rd Ed. (Ch. 3â€“4)  
- Chen & Goodman (1999). *An Empirical Study of Smoothing Techniques for Language Modeling.*

---

## ğŸ‘¨â€ğŸ’» Author
**Sungbum Kim**  
Ph.D. Student, Statistics @ UTD  
Assignment for **CS 6320 â€” Natural Language Processing**
