# Assignment 1 ‚Äî N-gram Language Models

## üìò Overview
This assignment implements **Uni-gram** and **Bi-gram** language models to analyze text corpora and estimate sequence probabilities.  
You will also experiment with **add-k smoothing** and evaluate model quality using **Perplexity (PP)** on held-out data.

---

## üß† Objectives
1. Implement probabilistic language models based on **n-gram frequency counts**.  
2. Apply **smoothing techniques** to handle unseen word combinations.  
3. Compute **sentence probabilities** and **perplexity scores** on validation data.  
4. Compare model performance across different smoothing constants and n-gram orders.

---

## üìÇ Files Included
| File | Description |
|------|--------------|
| `train.txt` | Training corpus used to build n-gram counts. |
| `val.txt` | Validation corpus used to compute perplexity. |
| `assignment1.ipynb` | Jupyter notebook implementing the full pipeline (training, smoothing, evaluation). |

---

## ‚öôÔ∏è Implementation Summary

### 1Ô∏è‚É£ Uni-gram Model
Estimates word probabilities as:  
> **P(w·µ¢) = count(w·µ¢) / N**  
> where **N** is the total number of tokens in the training corpus.

---

### 2Ô∏è‚É£ Bi-gram Model
Estimates conditional probabilities as:  
> **P(w·µ¢ | w·µ¢‚Çã‚ÇÅ) = count(w·µ¢‚Çã‚ÇÅ, w·µ¢) / count(w·µ¢‚Çã‚ÇÅ)**  

This measures how likely word *w·µ¢* follows *w·µ¢‚Çã‚ÇÅ* based on observed pairs.

---

### 3Ô∏è‚É£ Add-k (Laplace) Smoothing
Handles zero probabilities using a constant *k > 0*:  
> **P(w·µ¢ | w·µ¢‚Çã‚ÇÅ) = (count(w·µ¢‚Çã‚ÇÅ, w·µ¢) + k) / (count(w·µ¢‚Çã‚ÇÅ) + kV)**  
> where **V** is the vocabulary size.

Smoothing ensures that unseen word pairs receive small non-zero probabilities,  
preventing total probability collapse for unseen events.

---

### 4Ô∏è‚É£ Perplexity (PP)
Evaluates model predictive performance on unseen data:  
> **PP(W) = 2^(-1/N √ó Œ£ log‚ÇÇ P(w·µ¢ | w·µ¢‚Çã‚ÇÅ))**  

Lower perplexity ‚Üí better model (indicating higher average likelihood on test data).

---

## üß© Experiment Plan
| Model | Smoothing k | Evaluation Metric | Expected Trend |
|--------|--------------|------------------|----------------|
| Uni-gram | 0 | High PP | Baseline |
| Uni-gram | 0.1, 0.01, 0.001 | Moderate PP | Smaller PP than baseline |
| Bi-gram | 0 | Overfitted PP | Better than Uni-gram |
| Bi-gram | 0.1, 0.01, 0.001 | PP decreases | Optimal k near small positive value |

---

## üöÄ How to Run
1. Open `assignment1.ipynb` in Jupyter Notebook or VSCode.  
2. Run all cells in order.  
3. The notebook will:
   - Build n-gram counts from `train.txt`
   - Apply smoothing
   - Compute perplexity on `val.txt`
4. The output will show tables comparing PP across settings.

---

## üßæ Example Output
| Model | k | Perplexity |
|--------|---|-------------|
| Uni-gram | ‚Äì | 293.23 |
| Bi-gram | 0 | 2156.58 |
| Bi-gram | 0.1 | 185.06 |
| Bi-gram | 1.0 | 429.08 |

---

## ‚ú® Discussion Points
- Analyze how **smoothing** improves generalization on unseen words.  
- Compare **Uni-gram vs Bi-gram** behavior on rare tokens.  
- Comment on trade-off between model complexity and data sparsity.

---
