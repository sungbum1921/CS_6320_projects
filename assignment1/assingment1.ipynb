{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69f1131-6b21-4cd1-80d2-c7f6ec8a1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba24a96-fcb5-4d09-846d-cfbbb5a72120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Preprocessing ---\n",
    "class Corpus:\n",
    "    def __init__(self, train_path, val_path, unk_threshold=1):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.unk_threshold = unk_threshold\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.train_corpus_unk = []\n",
    "        self.val_corpus_unk = []\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        print(\"Loading and preparing data...\")\n",
    "        # 1. Load and tokenize sentences from the file\n",
    "        raw_train_corpus = self._preprocess_file(self.train_path)\n",
    "        raw_val_corpus = self._preprocess_file(self.val_path)\n",
    "\n",
    "        # 2. Create vocabulary from training data and handle <UNK> tokens\n",
    "        self.vocab, self.train_corpus_unk = self._handle_unknowns(raw_train_corpus)\n",
    "\n",
    "        # 3. Replace words in validation data with <UNK> based on the created vocabulary\n",
    "        self.val_corpus_unk = self._replace_oov(raw_val_corpus, self.vocab)\n",
    "        print(\"Data preparation complete.\")\n",
    "\n",
    "    def _preprocess_file(self, file_path):\n",
    "        processed_sentences = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.lower().strip().split()\n",
    "                if tokens:\n",
    "                    processed_sentences.append(['<s>'] + tokens + ['</s>'])\n",
    "        return processed_sentences\n",
    "\n",
    "    def _handle_unknowns(self, corpus):\n",
    "        # Counts word frequencies and replaces words below a threshold with <UNK>.\n",
    "        # Count the frequency of all words\n",
    "        word_counts = Counter(word for sentence in corpus for word in sentence)\n",
    "\n",
    "        # Add only words with a frequency greater than the threshold to the vocabulary dictionary.\n",
    "        vocab = {word for word, count in word_counts.items() if count > self.unk_threshold}\n",
    "        vocab.update(['<s>', '</s>', '<UNK>'])\n",
    "        \n",
    "        processed_corpus = [[word if word in vocab else '<UNK>' for word in sentence] for sentence in corpus]\n",
    "        return vocab, processed_corpus\n",
    "    \n",
    "    def _replace_oov(self, corpus, vocab):\n",
    "        # Replaces words not in the vocabulary with <UNK>.\n",
    "        return [[word if word in vocab else '<UNK>' for word in sentence] for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09aaba93-f7a8-44e6-b0ce-deb4dad55ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. N-gram Language Model Class (based on your provided logic) ---\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    def __init__(self, n=1, k=0):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"Trains the language model on the given corpus.\"\"\"\n",
    "        # print(\"Training model...\")\n",
    "        for sentence in corpus:\n",
    "            self.total_tokens += len(sentence)\n",
    "            # --- Unigram Counts ---\n",
    "            for word in sentence:\n",
    "                self.unigram_counts[word] += 1\n",
    "\n",
    "            # --- Bigram Counts ---\n",
    "            if self.n >= 2:\n",
    "                for i in range(len(sentence) - 1):\n",
    "                    bigram = (sentence[i], sentence[i+1])\n",
    "                    self.bigram_counts[bigram] += 1\n",
    "        \n",
    "        self.vocab_size = len(self.unigram_counts)\n",
    "        # print(f\"Training complete. Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "    def get_smoothed_unigram_prob(self, word):\n",
    "        \"\"\"Calculates smoothed unigram probability.\"\"\"\n",
    "        numerator = self.unigram_counts.get(word, 0) + self.k\n",
    "        denominator = self.total_tokens + (self.k * self.vocab_size)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def get_smoothed_bigram_prob(self, prev_word, word):\n",
    "        \"\"\"Calculates Add-k smoothed bigram probability.\"\"\"\n",
    "        bigram = (prev_word, word)\n",
    "        numerator = self.bigram_counts.get(bigram, 0) + self.k\n",
    "        denominator = self.unigram_counts.get(prev_word, 0) + (self.k * self.vocab_size)\n",
    "        # Handle case where the context (prev_word) was never seen\n",
    "        if denominator == 0:\n",
    "            return 1 / self.vocab_size\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def calculate_perplexity(self, validation_corpus):\n",
    "        \"\"\"Calculates perplexity for a unigram model.\"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        # M is the number of words / m is the number of sentences\n",
    "        M = 0\n",
    "\n",
    "        for sentence in validation_corpus:\n",
    "            # Total tokens in validation set, excluding <s> start tokens\n",
    "            M += len(sentence) - 1\n",
    "            for i in range(1, len(sentence)):\n",
    "                if self.n == 1 :\n",
    "                    word = sentence[i]\n",
    "                    prob = self.get_smoothed_unigram_prob(word)\n",
    "\n",
    "                if self.n == 2 :\n",
    "                    prev_word = sentence[i-1]\n",
    "                    word = sentence[i]\n",
    "                    prob = self.get_smoothed_bigram_prob(prev_word, word)\n",
    "                    \n",
    "                if prob > 0:\n",
    "                    total_log_prob += math.log2(prob)\n",
    "                else:\n",
    "                    total_log_prob += math.log2(1e-10)\n",
    "                \n",
    "        if M == 0:\n",
    "            return float('inf')\n",
    "                    \n",
    "        l = total_log_prob / M\n",
    "        perplexity = 2 ** (-l)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8cec91-6ec7-420a-9b99-77749b29110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Code ---\n",
    "train_path = '/train.txt' # your path for training data \n",
    "val_path = '/val.txt' # your path for validation data\n",
    "\n",
    "corpus = Corpus(train_path, val_path, unk_threshold = 1)\n",
    "corpus.load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d0aa760-78f2-4d4c-b3e8-2e2a64d07d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05834105040349252\n",
      "0.011388190677779247\n"
     ]
    }
   ],
   "source": [
    "model = NgramLanguageModel(n=1, k=0)\n",
    "model.train(corpus.train_corpus_unk)\n",
    "print(model.get_smoothed_unigram_prob('the'))\n",
    "print(model.get_smoothed_unigram_prob('hotel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbfa2fca-b357-4d31-b1f4-21c7f1b7d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07804232804232804\n",
      "0.24157303370786518\n"
     ]
    }
   ],
   "source": [
    "model = NgramLanguageModel(n=2, k=0)\n",
    "model.train(corpus.train_corpus_unk)\n",
    "print(model.get_smoothed_bigram_prob('the','hotel'))\n",
    "print(model.get_smoothed_bigram_prob('it','was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a9c1ab-bedc-4cf1-b04b-0162591641f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vec = [1, 2]\n",
    "k_vec = [1, 0.1, 0.01, 0.001, 0]\n",
    "\n",
    "results_train = np.zeros((len(n_vec) * len(k_vec), 3), dtype=float)\n",
    "idx = 0\n",
    "for n in n_vec:\n",
    "    for k in k_vec:\n",
    "        model = NgramLanguageModel(n=n, k=k)\n",
    "        model.train(corpus.train_corpus_unk)\n",
    "        ppl = model.calculate_perplexity(corpus.train_corpus_unk)\n",
    "        \n",
    "        results_train[idx] = [n, k, ppl]\n",
    "        idx += 1\n",
    "\n",
    "results_val = np.zeros((len(n_vec) * len(k_vec), 3), dtype=float)\n",
    "idx = 0\n",
    "for n in n_vec:\n",
    "    for k in k_vec:\n",
    "        model = NgramLanguageModel(n=n, k=k)\n",
    "        model.train(corpus.train_corpus_unk)\n",
    "        ppl = model.calculate_perplexity(corpus.val_corpus_unk)\n",
    "        \n",
    "        results_val[idx] = [n, k, ppl]\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f82a5f-f4cd-435b-9fd8-e3b4e74e347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LANGUAGE MODEL PERPLEXITY EVALUATION ##\n",
      "--- MODEL TRAINING SET EVALUATION ---\n",
      "--- Perplexity on Training Set ---\n",
      "Unigram | k = 1.000  | Perplexity: 336.5974\n",
      "Unigram | k = 0.100  | Perplexity: 335.6611\n",
      "Unigram | k = 0.010  | Perplexity: 335.6542\n",
      "Unigram | k = 0.001  | Perplexity: 335.6547\n",
      "Unigram | k = 0.000  | Perplexity: 335.6547\n",
      "Bigram  | k = 1.000  | Perplexity: 380.1728\n",
      "Bigram  | k = 0.100  | Perplexity: 108.1736\n",
      "Bigram  | k = 0.010  | Perplexity: 49.1018\n",
      "Bigram  | k = 0.001  | Perplexity: 35.8265\n",
      "Bigram  | k = 0.000  | Perplexity: 33.0065\n",
      "\n",
      "--- Perplexity on Validation Set ---\n",
      "Unigram | k = 1.000  | Perplexity: 295.4775\n",
      "Unigram | k = 0.100  | Perplexity: 293.3962\n",
      "Unigram | k = 0.010  | Perplexity: 293.2443\n",
      "Unigram | k = 0.001  | Perplexity: 293.2298\n",
      "Unigram | k = 0.000  | Perplexity: 293.2282\n",
      "Bigram  | k = 1.000  | Perplexity: 429.0841\n",
      "Bigram  | k = 0.100  | Perplexity: 185.0601\n",
      "Bigram  | k = 0.010  | Perplexity: 142.0454\n",
      "Bigram  | k = 0.001  | Perplexity: 175.9258\n",
      "Bigram  | k = 0.000  | Perplexity: 2156.5763\n"
     ]
    }
   ],
   "source": [
    "print(\"## LANGUAGE MODEL PERPLEXITY EVALUATION ##\")\n",
    "print(\"--- MODEL TRAINING SET EVALUATION ---\")\n",
    "print(\"--- Perplexity on Training Set ---\")\n",
    "\n",
    "for row in results_train :\n",
    "    if row[0] == 1 :\n",
    "        print(f\"Unigram | k = {row[1]:<6.3f} | Perplexity: {row[2]:.4f}\")    \n",
    "    if row[0] == 2 :\n",
    "        print(f\"Bigram  | k = {row[1]:<6.3f} | Perplexity: {row[2]:.4f}\")    \n",
    "\n",
    "print(\"\\n--- Perplexity on Validation Set ---\")\n",
    "for row in results_val :\n",
    "    if row[0] == 1 :\n",
    "        print(f\"Unigram | k = {row[1]:<6.3f} | Perplexity: {row[2]:.4f}\")    \n",
    "    if row[0] == 2 :\n",
    "        print(f\"Bigram  | k = {row[1]:<6.3f} | Perplexity: {row[2]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77f330-3498-4815-a8d4-f80c617ae342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51fee8-f5cd-4c30-9c6c-24c46338456a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22713658-86ce-40cc-8c93-5e38b19372d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771105bc-f196-4622-aabf-cfa3e367ba83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb89cd3-e6a7-41b2-9123-6f4e41d19f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba77aee-5126-46b9-99d5-8351f89b96fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661a6cd-683f-4fbd-bed6-1c71852938de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dd78a-c55c-4da8-a2ff-88929b74484e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9392c-0a8a-4d7a-8a8d-8b0c7d41f451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeb315-894b-4d84-a67c-057cba17e022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
